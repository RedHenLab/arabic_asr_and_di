{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dialect_enrollment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from utils.read_ivectors import read_ivecs_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ivecs_dir_path = '/home/ai/Projects/dialectID/data/train.vardial2017'\n",
    "dev_ivecs_dir_path = '/home/ai/Projects/dialectID/data/dev.vardial2017'\n",
    "dialects = ['EGY', 'GLF', 'LAV', 'MSA', 'NOR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(y_true, cos_sim):\n",
    "    '''Euclidean distance loss function.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    \n",
    "    y_true : 1-D NumPy array containing true labels.\n",
    "    \n",
    "    cos_sim : 1-D tensor containing cosine similarity scores.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    euclid_dist : 0-D tensor of Euclidean distance between true labels\n",
    "    and cosine similarity scores.\n",
    "    '''\n",
    "    euclid_dist = keras.backend.sum(keras.backend.square(y_true - \n",
    "        (1 - cos_sim)))\n",
    "    return euclid_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_network(input_layer):\n",
    "    '''Build the identical part constituting the Siamese NN branches.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    \n",
    "    input_layer : keras.layers.Input object.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    fc_3 : keras.layers.Dense object with 200 nodes.\n",
    "    '''\n",
    "    if len(input_layer.shape) == 2:\n",
    "        in_shape = input_layer.shape[1]\n",
    "    else:\n",
    "        raise ValueError(('Expected shape (?,n). '\n",
    "            'Found shape {} is not right.').format(input_layer.shape))\n",
    "    input_reshaped = keras.layers.Reshape((in_shape,1))(input_layer)\n",
    "    conv = keras.layers.Conv1D(filters=25, kernel_size=8,\n",
    "                                 activation='relu',\n",
    "                                padding='same')(input_reshaped)\n",
    "    flat = keras.layers.Flatten()(conv)\n",
    "    fc_1 = keras.layers.Dense(1500, activation='relu')(flat)\n",
    "    fc_2 = keras.layers.Dense(600, activation='relu')(fc_1)\n",
    "    fc_3 = keras.layers.Dense(200, activation='relu')(fc_2)\n",
    "    return fc_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read i-vectors for training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ivecs = dialect_enrollment.read_ivecs_set(train_ivecs_dir_path,\n",
    "    dialects)\n",
    "dev_ivecs = dialect_enrollment.read_ivecs_set(dev_ivecs_dir_path,\n",
    "    dialects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute dialect enrollment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_model = dialect_enrollment.model(train_ivecs, dev_ivecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset by randomly choosing utterances and a dialect enrollment model, and deducing the corresponding dialect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_ivecs.sample(1000)\n",
    "x_train = x_train.append(dev_ivecs.sample(50))\n",
    "# Make sure all models have the same dimensionality\n",
    "model_lens = set(len(model) for model in de_model.values())\n",
    "assert len(model_lens) == 1\n",
    "# Form the other part of the training data from the model and append\n",
    "# it to the training data i-vectors\n",
    "de_model_df = pd.DataFrame([v.tolist() + [k] for k, v in\n",
    "    de_model.items()], columns=list(range(model_lens.pop())) \n",
    "    + ['model_dialect'])\n",
    "x_train_model = de_model_df.sample(1050, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reset_index(drop=True)\n",
    "x_train_model = x_train_model.reset_index(drop=True)\n",
    "y = pd.concat([x_train['dialect'],\n",
    "    x_train_model['model_dialect']], axis=1)\n",
    "y['label'] = y.apply(lambda row: 1 if row['dialect'] ==\n",
    "    row['model_dialect'] else 0, axis=1)\n",
    "y = y['label'].values\n",
    "x_train = x_train.drop('dialect', axis=1)\n",
    "x_train_model = x_train_model.drop('model_dialect', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = keras.layers.Input(shape=(400,))\n",
    "input_2 = keras.layers.Input(shape=(400,))\n",
    "# Create base network for the Siamese neural network\n",
    "base_net_1 = base_network(input_1)\n",
    "base_net_2 = base_network(input_2)\n",
    "# Create Siamese neural network\n",
    "merged = keras.layers.Dot(normalize=True, axes=1)(\n",
    "    [base_net_1, base_net_2])\n",
    "model = keras.Model(inputs=[input_1, input_2], outputs=merged)\n",
    "model.compile(loss=euclidean_distance, optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a callback function to save the model every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = 'model.epoch{:02d}-{val_acc:.2f}.hdf5'\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(model_file_path,\n",
    "    monitor='val_acc', verbose=1, save_best_only=True)\n",
    "batch_progress_callback = keras.callbacks.LambdaCallback(\n",
    "    on_batch_begin=lambda batch,logs:print('Batch {}'.format(batch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([x_train, x_train_model], y,\n",
    "    epochs=20, batch_size=50, callbacks=[checkpoint_callback,\n",
    "    batch_progress_callback])\n",
    "\n",
    "print(\"Finished training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
